{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y9mx5erQJrn"
      },
      "source": [
        "Jalankan kode ini jika kita menggunakan Google Colab. Untuk prediksi yang lebih cepat, disarankan mengaktifkan mode GPU di Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UoGs4SZS06Q",
        "outputId": "7b3e30a4-5fe0-4ff8-8ab5-0bece29174bd"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/karuniaperjuangan/Twitter-Sentiment-Analysis-NLP-Pra-UTS.git\n",
        "import os\n",
        "os.chdir(\"Twitter-Sentiment-Analysis-NLP-Pra-UTS\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3N38cPKbYUnF",
        "outputId": "b6645b31-4860-4019-e3b0-4cd7eeb7d3b8"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWyQR4YfQJrr"
      },
      "source": [
        "# Tugas NLP Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pada tugas kali ini, kita akan melakukan sentiment Analysis terhadap teks yang discrape dari Twitter. Model yang kita gunakan adalah DistilBERT yang sudah ditraining dalam [Notebook yang berbeda](https://github.com/karuniaperjuangan/Twitter-Sentiment-Analysis-NLP-Pra-UTS/blob/main/training/Training%20Model.ipynb) dengan dataset Sentiment Analysis IndoNLU yang fokus pada bahasa Indonesia. Alasan penggunaan DistilBERT ada di slide presentasi. Kita di sini lebih fokus ke inferencing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import beberapa Library yang diperlukan dahulu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXraelR2QJrr"
      },
      "outputs": [],
      "source": [
        "import gradio as gr # Untuk UI\n",
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import gc\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import datetime as dt\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gunakan GPU jika memungkinkan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqpAkaQmQJrr"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available\")\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    !nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1-VbuZ0QJrs"
      },
      "source": [
        "# Scraping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Kita akan melakukan scraping Twitter dengan bantuan SNScrape. Kita akan lebih fokus ke isi teks Twitter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6QCszMnQJrs"
      },
      "outputs": [],
      "source": [
        "def scrape_tweets(query, max_tweets=-1,output_path=\"./scraper/output/\" ): \n",
        "    output_path = os.path.join(output_path,dt.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")+\"-\"+str(query)+\".csv\")\n",
        "    \n",
        "    tweets_list = []\n",
        "    if sys.version_info.minor>=8:\n",
        "        try:\n",
        "            for i,tweet in tqdm(enumerate(sntwitter.TwitterSearchScraper(query).get_items())):\n",
        "                if max_tweets != -1 and i >= int(max_tweets):\n",
        "                    break\n",
        "                tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username, tweet.likeCount, tweet.retweetCount, tweet.replyCount, tweet.quoteCount, tweet.url, tweet.lang])\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"Scraping berhenti atas permintaan pengguna\")\n",
        "        \n",
        "        \n",
        "        df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username', 'Likes', 'Retweets', 'Replies', 'Quotes', 'URL', 'Language'])\n",
        "        print(\"Tweet berbahasa Indonesia :\",len(df[df[\"Language\"] == \"in\"]),\"/\",len(tweets_list))\n",
        "        df = df[df[\"Language\"] == \"in\"]\n",
        "    #Karena Google Colab menggunakan versi 3.7, library scrape yang digunakan adalah versi lawas yang tidak lengkap, sehingga kita tidak bisa melakukan filter bahasa Indonesia\n",
        "    else:\n",
        "        print(\"Using older version of Python\")\n",
        "        try:\n",
        "            for i,tweet in tqdm(enumerate(sntwitter.TwitterSearchScraper(query).get_items())):\n",
        "                if max_tweets != -1 and i >= int(max_tweets):\n",
        "                    break\n",
        "                tweets_list.append([tweet.date, tweet.id, tweet.content])\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"Scraping berhenti atas permintaan pengguna\")\n",
        "        df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text'])\n",
        "    \n",
        "    df.to_csv(output_path, index=False)\n",
        "    print(\"Data tweet tersimpan di\",output_path)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agar lebih akurat dan lebih ringan, kita perlu melakukan *preprocessing* terlebih dahulu. Caranya adalah menghapus tag mention, URL, dan segala karakter non alfanumerik seperti tanda baca dan emoji."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_unnecessary_char(text):\n",
        "  text = re.sub(\"\\[USERNAME\\]\", \" \", text)\n",
        "  text = re.sub(\"\\[URL\\]\", \" \", text)\n",
        "  text = re.sub(\"\\[SENSITIVE-NO\\]\", \" \", text)\n",
        "  text = re.sub('  +', ' ', text)\n",
        "  return text\n",
        "\n",
        "def preprocess_tweet(text):\n",
        "  text = re.sub('\\n',' ',text) # Remove every '\\n'\n",
        "  # text = re.sub('rt',' ',text) # Remove every retweet symbol\n",
        "  text = re.sub('^(\\@\\w+ ?)+',' ',text)\n",
        "  text = re.sub(r'\\@\\w+',' ',text) # Remove every username\n",
        "  text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))',' ',text) # Remove every URL\n",
        "  text = re.sub('/', ' ', text)\n",
        "  # text = re.sub(r'[^\\w\\s]', '', text)\n",
        "  text = re.sub('  +', ' ', text) # Remove extra spaces\n",
        "  return text\n",
        "    \n",
        "def remove_nonaplhanumeric(text):\n",
        "  text = re.sub('[^0-9a-zA-Z]+', ' ', text) \n",
        "  return text\n",
        "\n",
        "def preprocess_text(text):\n",
        "  text = preprocess_tweet(text)\n",
        "  text = remove_unnecessary_char(text)\n",
        "  text = remove_nonaplhanumeric(text)\n",
        "  text = text.lower()\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Kemudian kita bisa menggunakan pipeline `text-classification` dengan model yang sudah dibuat dan tersimpan sebagai model `karuniaperjuangan/smsa-distilbert-indo` di HuggingFace. Analisis sentimen ini dilakukan setelah proses scraping dan di akhir kita juga akan menampilkan plot Pie Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdaIuG7qQJrt"
      },
      "outputs": [],
      "source": [
        "predict = pipeline('text-classification',\n",
        "                   model='karuniaperjuangan/smsa-distilbert-indo',\n",
        "                    device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "def analyze_df_sentiment(df, batch_size):\n",
        "    text_list = list(df[\"Text\"].astype(str).values)\n",
        "    text_list_batches = [text_list[i:i+batch_size] for i in range(0,len(text_list),batch_size)] # Memisahkan berdasar batch size dengan bantuan zip ()\n",
        "    \n",
        "    predictions = []\n",
        "    for batch in tqdm(text_list_batches):\n",
        "        batch_predictions = predict(batch)\n",
        "        predictions += batch_predictions\n",
        "    df[\"Label\"] = [pred[\"label\"] for pred in predictions]\n",
        "    df[\"Score\"] = [pred[\"score\"] for pred in predictions]\n",
        "    \n",
        "    return df\n",
        "\n",
        "def keyword_analyzer(keyword, max_tweets, batch_size=16):\n",
        "    print(\"Scraping tweets...\")\n",
        "    df = scrape_tweets(keyword, max_tweets=max_tweets)\n",
        "    df[\"Text\"] = df[\"Text\"].apply(preprocess_text)\n",
        "    print(\"Analyzing sentiment...\")\n",
        "    df = analyze_df_sentiment(df, batch_size=batch_size)\n",
        "    fig = plt.figure()\n",
        "    df.groupby([\"Label\"])[\"Text\"].count().plot.pie(autopct=\"%.1f%%\", figsize=(6,6))\n",
        "    return fig, df[[\"Text\", \"Label\", \"Score\"]]\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtdYnHcrQJru"
      },
      "source": [
        "# Web UI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Kita di sini akan menjalankan aplikasi yang memiliki Web UI dengan bantuan Gradio. Kita bisa mencari berdasarkan Keyword yang ingin kita cari atau juga bisa mencoba dengan memasukkan satu teks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R73wroSmQJrv",
        "outputId": "84e33717-64dc-4171-f211-e47cd447e6f6"
      },
      "outputs": [],
      "source": [
        "with gr.Blocks() as demo:\n",
        "    \n",
        "\n",
        "\n",
        "    gr.Markdown(\"\"\"<h1 style=\"text-align:center\">Aplikasi Sentiment Analysis Keyword Twitter </h1>\"\"\")\n",
        "    \n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        Aplikasi ini digunakan untuk melakukan sentimen analisis terhadap data di Twitter menggunakan model DistilBERT. Terdapat 2 mode yang dapat digunakan:\n",
        "        1. Trend/Keyword: Untuk melakukan analisis terhadap semua tweet yang mengandung keyword yang diinputkan\n",
        "        2. Tweet: Untuk melakukan analisis terhadap sebuah tweet yang diinputkan\n",
        "        \"\"\"\n",
        "        \n",
        "        )\n",
        "    with gr.Tab(\"Trend/Keyword\"):\n",
        "        gr.Markdown(\"\"\"Masukkan keyword dan jumlah maksimum tweet yang ingin diambil\"\"\")\n",
        "        with gr.Blocks():\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    keyword_textbox = gr.Textbox(lines=1, label=\"Keyword\")\n",
        "                    max_tweets_component = gr.Number(value=-1, label=\"Tweet Maksimal yang akan discrape (-1 jika ingin mengscrape semua tweet)\", precision=0)\n",
        "                    batch_size_component = gr.Number(value=16, label=\"Batch Size (Semakin banyak semakin cepat, tetapi semakin boros memori)\", precision=0)\n",
        "                    button = gr.Button(\"Submit\")\n",
        "                    \n",
        "                plot_component = gr.Plot(label=\"Pie Chart\")\n",
        "            dataframe_component = gr.DataFrame(type=\"pandas\", \n",
        "                                               label=\"Dataframe\", \n",
        "                                               max_rows=(20,'fixed'), \n",
        "                                               overflow_row_behaviour='paginate', \n",
        "                                               wrap=True)\n",
        "\n",
        "        \n",
        "        \n",
        "    with gr.Tab(\"Single Tweet\"):\n",
        "        gr.Interface(lambda Tweet: (predict(Tweet)[0]['label'], predict(Tweet)[0]['score']), \n",
        "                     \"textbox\",\n",
        "                     [\"label\", \"label\"],\n",
        "                     allow_flagging='never',\n",
        "                     )\n",
        "    \n",
        "    \n",
        "    gr.Markdown(\n",
        "            \"\"\"\n",
        "            \n",
        "            ## Anggota Kelompok\n",
        "            \n",
        "            - Karunia Perjuangan Mustadl'afin - 20/456368/TK/50498\n",
        "            \n",
        "            - Pramudya Kusuma Hardika - 20/460558/TK/51147\n",
        "            \n",
        "            \"\"\"\n",
        "            \n",
        "        )\n",
        "    \n",
        "    button.click(keyword_analyzer, \n",
        "                inputs=[keyword_textbox, max_tweets_component, batch_size_component], \n",
        "                outputs=[plot_component, dataframe_component])\n",
        "\n",
        "demo.launch(inbrowser=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "81bd2ac432b5952ddf1c49649eb1e47a752045e4beee075d8314834b79b876ce"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
